{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5 - Brian Hicks, Joe Everton\n",
    "## CS 498, Applied Machine Learning\n",
    "Using the ADL dataset, available [here](https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.metrics.pairwise import paired_euclidean_distances, euclidean_distances\n",
    "# import scipy.spatial.distance\n",
    "# from scipy.linalg import eigh as largest_eigh\n",
    "# import sklearn.metrics\n",
    "# from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ntpath\n",
    "# https://stackoverflow.com/questions/8384737/extract-file-name-from-path-no-matter-what-the-os-path-format\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def load_data():\n",
    "  dataset = []\n",
    "  labels = []\n",
    "  label_index = 0\n",
    "  for dirname, dirnames, filenames in os.walk('HMP_Dataset'):\n",
    "      # target all leaf directories not ending with '_MODEL'.\n",
    "      if len(dirnames) == 0 and '_MODEL' not in dirname:\n",
    "        class_name = path_leaf(dirname)\n",
    "        labels.append(class_name)\n",
    "#         print(\"dirname: {}, filenames: {}\".format(dirname, len(filenames)))\n",
    "        for filename in filenames:\n",
    "#             print(os.path.join(dirname, filename))\n",
    "            dataset.append((np.loadtxt(os.path.join(dirname, filename)), label_index))\n",
    "        label_index += 1\n",
    "  return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_flatten_example(example, k, offset):\n",
    "  chunk_list = np.array_split(example, np.arange(k, len(data), step=k))\n",
    "  if len(chunk_list[-1]) != k:\n",
    "    chunk_list = chunk_list[:-1]\n",
    "  if offset != k:\n",
    "    chunk_list = chunk_list[1:]\n",
    "  chunk_list = [np.ravel(chunk) for chunk in chunk_list]\n",
    "  return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class h_kmeans:\n",
    "  n_jobs = 2\n",
    "  def __init__(self, k_breadth):\n",
    "    # k_breadth is a vector of K for each depth.  The depth of the tree == length of k_breadth.\n",
    "    self.my_k = k_breadth\n",
    "    print (\"Initializing KMeans, n_clusters = {}\".format(self.my_k[0]))\n",
    "    self.km = KMeans(n_clusters=self.my_k[0], n_jobs=-1)\n",
    "    self.children = []\n",
    "    if len(k_breadth) > 1:\n",
    "      # Remove my k, pass the rest to the children.\n",
    "      self.children = [h_kmeans(k_breadth[1:]) for km in range(self.my_k[0])]\n",
    "      # self.children = [h_kmeans(k_breadth[1:])] * self.my_k[0]\n",
    "\n",
    "  def fit(self, X):\n",
    "    self.km.fit(X)\n",
    "    if len(self.children) > 0:\n",
    "      clusters = self.km.predict(X)\n",
    "      for index, child_node in enumerate(self.children):\n",
    "        # print(clusters == index)\n",
    "        child_node.fit(X[clusters == index])\n",
    "  \n",
    "  def predict(self, X):\n",
    "    if len(self.children) == 0:\n",
    "      clusters = self.km.predict(X)\n",
    "      return clusters\n",
    "    if len(self.children) > 0:\n",
    "      clusters = self.km.predict(X)\n",
    "      depth_clusters = []\n",
    "      for i in range(len(X)):\n",
    "        depth_clusters.append(\n",
    "          clusters[i] * self.myk[1] + self.children[clusters[i]].predict(X[i]))\n",
    "      return depth_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut into k-sized chunks and fill the dictionary.\n",
    "k = 32\n",
    "# TODO: dictionary is a poor name for this.\n",
    "dictionary = []\n",
    "for example in dataset:\n",
    "  data = example[0]\n",
    "  for offset in range(0, k, 11): # 0, 11, 22, to get about 3 x 13544 chunks.\n",
    "    split_data = split_and_flatten_example(data, k, k - offset)\n",
    "    # print(\"example size: {} split into {} chunks\".format(data.shape,len(split_data)))\n",
    "    dictionary.extend(split_data)\n",
    "dictionary = np.array(dictionary) # convert from list of arrays to 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-means\n",
    "# km = KMeans(n_clusters=480, random_state=0, n_jobs=2).fit(dictionary)\n",
    "km = h_kmeans((40,12))\n",
    "km.fit(dictionary)\n",
    "# km.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdict = dictionary[:20]\n",
    "clusters = km.predict(subdict)\n",
    "for i, cluster in enumerate(clusters[clusters >= 400]):\n",
    "  print(\"i {} cluster {}\".format(i, cluster))\n",
    "  cluster = -1\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(dataset[0][0].shape)\n",
    "train, test = train_test_split(dataset, test_size=0.33)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = np.ones((2167,3))\n",
    "my_split = np.array_split(data, np.arange(32,len(data),step=32))\n",
    "len(my_split[-2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
