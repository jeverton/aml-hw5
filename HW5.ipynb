{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5 - Brian Hicks, Joe Everton\n",
    "## CS 498, Applied Machine Learning\n",
    "Using the ADL dataset, available [here](https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ntpath\n",
    "# https://stackoverflow.com/questions/8384737/extract-file-name-from-path-no-matter-what-the-os-path-format\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def load_data():\n",
    "  dataset = []\n",
    "  labels = []\n",
    "  label_index = 0\n",
    "  for dirname, dirnames, filenames in os.walk('HMP_Dataset'):\n",
    "      # target all leaf directories not ending with '_MODEL'.\n",
    "      if len(dirnames) == 0 and '_MODEL' not in dirname:\n",
    "        class_name = path_leaf(dirname)\n",
    "        labels.append(class_name)\n",
    "        for filename in filenames:\n",
    "            dataset.append((np.loadtxt(os.path.join(dirname, filename)), label_index))\n",
    "        label_index += 1\n",
    "  return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_stack(a, stepsize, width):\n",
    "  n = a.shape[0]\n",
    "  return np.hstack( a[i:1+n+i-width:stepsize] for i in range(0,width) )\n",
    "\n",
    "def get_flattened_data(data, stepsize, width):\n",
    "  flattened_data = window_stack(data[0][0], stepsize, width)\n",
    "  for i in range(1, len(dataset)):\n",
    "    slices = window_stack(data[i][0], stepsize, width)\n",
    "    flattened_data = np.concatenate((flattened_data, slices), axis=0)\n",
    "  return flattened_data\n",
    "\n",
    "# def split_and_flatten_example(example, k, stepsize):\n",
    "#   chunk_list = window_stack(example, stepsize, k)\n",
    "# #   chunk_list = np.array_split(example, np.arange(k, len(example), step=k))\n",
    "#   if len(chunk_list[-1]) != k:\n",
    "#     chunk_list = chunk_list[:-1]\n",
    "#   if stepsize != k:\n",
    "#     chunk_list = chunk_list[1:]\n",
    "# #   chunk_list = [np.ravel(chunk) for chunk in chunk_list]\n",
    "#   return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class h_kmeans:\n",
    "  n_jobs = -1\n",
    "  def __init__(self, n_clusters, n_jobs=-1):\n",
    "    # n_clusters is a vector of K for each depth.  The depth of the tree == length of n_clusters.\n",
    "    if type(n_clusters) != tuple:\n",
    "      # if given a non-tuple (int) make it a tuple.\n",
    "      n_clusters = (n_clusters,)\n",
    "    h_kmeans.n_jobs = n_jobs\n",
    "    self.my_k = n_clusters\n",
    "    # print (\"Initializing KMeans, n_clusters = {}\".format(self.my_k[0]))\n",
    "    self.km = KMeans(n_clusters=self.my_k[0], n_jobs=h_kmeans.n_jobs)\n",
    "    self.children = []\n",
    "    if len(n_clusters) > 1:\n",
    "      # Remove my k, pass the rest to the children.\n",
    "      self.children = [h_kmeans(n_clusters[1:]) for km in range(self.my_k[0])]\n",
    "      # self.children = [h_kmeans(n_clusters[1:])] * self.my_k[0]\n",
    "\n",
    "  def fit(self, X):\n",
    "    if len(X) > 5000 and len(self.children) > 0:\n",
    "      rand_indexes = np.random.randint(len(X), size=5000)\n",
    "      self.km.fit(X[rand_indexes])\n",
    "    else:\n",
    "      self.km.fit(X)\n",
    "    if len(self.children) > 0:\n",
    "      clusters = self.km.predict(X)\n",
    "      # print(\"fitting children with row count:\", end='')\n",
    "      for index, child_node in enumerate(self.children):\n",
    "        # pick out the rows that were predicted for cluster child_node.\n",
    "        match = clusters == index\n",
    "        # print(\"{}, \".format(np.sum(match)), end='')\n",
    "        child_node.fit(X[match])\n",
    "      # print(\"\")\n",
    "    return self\n",
    "  \n",
    "  def predict(self, X):\n",
    "    if len(self.children) == 0:\n",
    "      clusters = self.km.predict(X)\n",
    "      return clusters\n",
    "    if len(self.children) > 0:\n",
    "      clusters = self.km.predict(X)\n",
    "      depth_clusters = []\n",
    "      # TODO: We could pass sample_weight, and have it be 0 where it doesn't \n",
    "      # belong to this child, and 1 where it does.  That would get rid of the\n",
    "      # for loop.\n",
    "      for i in range(len(X)):\n",
    "        # a cluster anywhere on the tree should have a unique id for histogramming\n",
    "        # This math is supposed to use children results to get the flattened cluster\n",
    "        # index.\n",
    "        depth_clusters.append(\n",
    "          clusters[i] * self.my_k[1] + self.children[clusters[i]].predict(X[i].reshape(1, -1)).item())\n",
    "      return np.array(depth_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histograms(km, dataset, K, k):\n",
    "  hist_points = []\n",
    "  for example in dataset:\n",
    "    split_image = window_stack(example[0], 11, k)\n",
    "    clusters = km.predict(split_image)\n",
    "    hist, bin_edges = np.histogram(clusters, density=True, bins=np.arange(K+1))\n",
    "    hist_points.append(hist)\n",
    "  hist_points = np.array(hist_points)\n",
    "  class_labels = np.array(list(zip(*dataset))[1])\n",
    "  return hist_points, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy_with_folding(hist_points, class_labels):\n",
    "  accuracies = []\n",
    "  kf = KFold(n_splits=3, shuffle=True)\n",
    "  for train_index, test_index in kf.split(hist_points, class_labels):\n",
    "    X_train, X_test = hist_points[train_index], hist_points[test_index]\n",
    "    y_train, y_test = class_labels[train_index], class_labels[test_index]\n",
    "    clf = RandomForestClassifier(n_estimators=750, max_depth=120)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_prediction = clf.predict(X_test)\n",
    "    accuracies.append(np.sum(y_prediction == y_test) / len(y_test))\n",
    "  print(\"accuracy mean: {}\\n\\tindividual: {}\".format(np.mean(accuracies), accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut into k-sized chunks and fill the dictionary.\n",
    "k = 32\n",
    "dictionary = get_flattened_data(dataset, 11, k)\n",
    "print(dictionary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# k_experiments = ((32, 15), 480, (40, 12))#, (15, 8, 4))\n",
    "# k_experiments = (100, 200, 300, 400, 480, 500)\n",
    "# k_experiments = (10, 25, 50, 100, 125, 150, 175, 200, 225, 250)\n",
    "# k_experiments = (200, 480)\n",
    "k_experiments = (400, 480, (32, 15), (40, 12), (15, 8, 4))\n",
    "for k_val in k_experiments:\n",
    "  print(\"k_val: {}\".format(k_val))\n",
    "  km = h_kmeans(k_val, n_jobs=-1).fit(dictionary)\n",
    "  hist_points, class_labels = get_histograms(km, dataset, np.prod(k_val), k)\n",
    "  measure_accuracy_with_folding(hist_points, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subdict = dictionary[:20]\n",
    "clusters = km.predict(subdict)\n",
    "for i, cluster in enumerate(clusters[clusters >= 400]):\n",
    "  print(\"i {} cluster {}\".format(i, cluster))\n",
    "  cluster = -1\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split experiments.\n",
    "Probably going to use kfold anyway."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(dataset[0][0].shape)\n",
    "train, test = train_test_split(dataset, test_size=0.33)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = np.ones((2167,3))\n",
    "my_split = np.array_split(data, np.arange(32,len(data),step=32))\n",
    "len(my_split[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with NDArray Views.\n",
    "Never did find a way to use integer indexing to produce a view."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def set_all_to_value(a, value):\n",
    "  a.fill(value)\n",
    "\n",
    "foo = np.tile(np.array(np.arange(5)), 5)\n",
    "set_all_to_value(foo.view()[[1, 2, 8, 11]], -1)\n",
    "foo[[1, 2, 8, 11]][2] = -1\n",
    "foo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# x = np.array([(1, 2),(3,4)], dtype=[('aasdf', np.int8), ('bwerewr', np.int8)])\n",
    "x = np.array([1, 2, 3, 4])# , dtype=[('aasdf', np.int8), ('bwerewr', np.int8)])\n",
    "print(x)\n",
    "#xv = x.view(dtype=np.int8).reshape(-1,2)\n",
    "xv = x.view()\n",
    "print(xv)\n",
    "y = xv[[1, 2],]\n",
    "y[1] = -20\n",
    "print(y)\n",
    "print(xv)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = np.array(5)\n",
    "b = np.array(6)\n",
    "c = np.array(7)\n",
    "v = []\n",
    "v.append(a)\n",
    "v.append(b.item())\n",
    "v.append(c.item())\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def window_stack(a, stepsize, width):\n",
    "  n = a.shape[0]\n",
    "  return np.hstack( a[i:1+n+i-width:stepsize] for i in range(0,width) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = dataset[0][0]\n",
    "# w = np.hstack((a[:-2],a[1:-1],a[2:]))\n",
    "# a = np.arange(64).reshape(2,32).T\n",
    "print(np.ravel(a))\n",
    "print(a)\n",
    "print(a.shape)\n",
    "# print(a[0:32:4])\n",
    "# print(a[1:32:4])\n",
    "# print(np.hstack([a[0:32:4], a[1:32:4]]))\n",
    "w = window_stack(a, 4, 32)\n",
    "print(w)\n",
    "print(w.shape)\n",
    "# a[1:5:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
